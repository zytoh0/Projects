{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Ames House Price - Advanced Regression Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. Introduction\n",
    "2. Exploratory Data Analysis\n",
    "2.1. Sale Price\n",
    "2.2. Numerical Features\n",
    "2.3. Categorical Features\n",
    "2.4. Correlations\n",
    "2.5. Missing Values\n",
    "3. Data Preprocessing and Feature Engineering\n",
    "3.1. Missing Values\n",
    "3.2. Outliers\n",
    "3.3. Feature Engineering\n",
    "3.3.1. Create New variables\n",
    "3.3.2. Label Encoding\n",
    "3.3.3. Transform Numerical Variables to Categorical Variables\n",
    "3.4. Skewness and Normalizing Variables\n",
    "3.5. Feature Scaling\n",
    "3.6. One-hot Encoding\n",
    "4. Modeling\n",
    "4.1. Regularized Regressions\n",
    "4.1.1. Ridge Regression\n",
    "4.1.2. Lasso Regression\n",
    "4.2. XGBoost\n",
    "4.3. LightGBM\n",
    "4.4. Averaging model\n",
    "5. Conclusion\n",
    "6. Reference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "After my first two data science projects solving classification problems, I am looking to expand my skill set by doing a comprehensive regression project. While taking Data Science micro-courses on Kaggle, I came across the House Prices: Advanced Regression Techniques competition. With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges me to predict the final price of each home.\n",
    "\n",
    "The Ames Housing dataset was compiled by Dean De Cock for use in data science education. The data set describing the sale of individual residential property in Ames, Iowa from 2006 to 2010. The data set contains 2930 observations and a large number of explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) involved in assessing home values.\n",
    "\n",
    "First, I performed comprehensive exploratory data analysis to understand linear relationships among the most important variables and detect potential issues such as sknewness, outliers and missing values. Then, I handled these issues, cleansed the data and performed feature engineering. Lastly, I built machine learning models to predict house prices. By the time I write this notebook, my best model has Mean Absolute Error of 12293.919, ranking 95/15502, approximately top 0.6% in the Kaggle leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose our data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are 3 datasets included in the [`data`](./data/) folder for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [`sample_sub_reg.csv`](./data/sample_sub_reg.csv): sample_sub_reg\n",
    "* [`test.csv`](./data/test.csv): test\n",
    "* [`train.csv`](./data/train): train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..data/test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-adac4dc43a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..data/test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..data/test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Separate features and target variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..data/test.csv'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data = pd.read_csv('..data/test.csv', index_col='Id')\n",
    "test_data = pd.read_csv('..data/test.csv', index_col='Id')\n",
    "\n",
    "# Separate features and target variable\n",
    "X_train = train_data.drop(['SalePrice'], axis=1)\n",
    "y = train_data.SalePrice\n",
    "\n",
    "# Concatenate train and test data\n",
    "X = pd.concat([X_train, test_data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data's size: \", X_train.shape)\n",
    "print(\"Test data's size: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCols = list(X_train.select_dtypes(exclude='object').columns)\n",
    "print(f\"There are {len(numCols)} numerical features:\\n\", numCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catCols = list(X_train.select_dtypes(include='object').columns)\n",
    "print(f\"There are {len(catCols)} numerical features:\\n\", catCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data dictionary can be found here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1. Sale Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot(y)\n",
    "title = plt.title(\"House Price Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The distribution of SalePrice is right-skewed. Let's check its Skewness and Kurtosis statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Skewness: {y.skew()}\n",
    "Kurtosis: {y.kurt()}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.2. Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top 10 numerical variables highly correlated with SalePrice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = train_data.corr().SalePrice.sort_values(ascending=False)\n",
    "corr_mat.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 10 features selected by Recursive Feature Elimination?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "estimator = LinearRegression()\n",
    "rfe = RFE(estimator, n_features_to_select=10, step=1)\n",
    "selector = rfe.fit(X_train.fillna(0).select_dtypes(exclude='object'), y)\n",
    "selectedFeatures = list(\n",
    "    X.select_dtypes(exclude='object').columns[selector.support_])\n",
    "selectedFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to above analyses, Overall Quality, Living Area, Number of Full Baths, Size of Garage and Year Built are some of the most important features in determining house price. Let's take a closer look at them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Quality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall quality is the most important feature in both analyses. It is clear that higher quality makes the house more expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='OverallQual', y='SalePrice', data=train_data, palette='GnBu')\n",
    "title = plt.title('House Price by Overall Quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Living Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Living area has a linear relationship with house price. In the scatter plot below, we can clearly see some outliers in the data, especially the two houses in the lower-right corner with living area greater than 4000 sqft and price lower than $200,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCorrelation(variables):\n",
    "    \"\"\"\n",
    "    1. Print correlation of two variables\n",
    "    2. Create jointplot of two variables\n",
    "    \"\"\"\n",
    "    # Print correlation\n",
    "    print(\"Correlation: \", train_data[[variables[0],\n",
    "                                       variables[1]]].corr().iloc[1, 0])\n",
    "\n",
    "    # Create jointplot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.jointplot(train_data[(variables[0])],\n",
    "                  train_data[(variables[1])],\n",
    "                  kind='reg',\n",
    "                  height=7,\n",
    "                  scatter_kws={'s': 10},\n",
    "                  marginal_kws={'kde': True})\n",
    "\n",
    "\n",
    "plotCorrelation(['GrLivArea', 'SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GarageCars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, houses with garage which can hold 4 cars are cheaper than houses with 3-car garage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='GarageCars', y='SalePrice', data=train_data, palette='GnBu')\n",
    "title = plt.title('House Price by Garage Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year Built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age of the house also plays an important role in its price. Newer houses have higher average prices. There are several houses built before 1900 having a high price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "sns.scatterplot(x='YearBuilt', y='SalePrice', data=train_data)\n",
    "title = plt.title('House Price by Year Built')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Using ANOVA, I have identified 15 categorical features having p-values lower than 0.01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigCatCols = [\n",
    "    'Street', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood',\n",
    "    'Condition1', 'Condition2', 'RoofMatl', 'ExterQual', 'BsmtQual',\n",
    "    'BsmtExposure', 'KitchenQual', 'Functional', 'GarageQual', 'PoolQC'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeCatFeature(feature):\n",
    "    \"\"\"\n",
    "    Visualize the relationship between `SalePrice` and categorical feature using box plots\n",
    "    \"\"\"\n",
    "    # Descending order of levels sorted by median SalePrice\n",
    "    featOrder = train_data.groupby(\n",
    "        [feature]).median().SalePrice.sort_values(ascending=False).index\n",
    "\n",
    "    # Create box plot\n",
    "    sns.boxplot(x=feature,\n",
    "                y='SalePrice',\n",
    "                data=train_data,\n",
    "                order=featOrder,\n",
    "                palette='GnBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a big difference in house prices among neighborhood in Ames. The top 3 expensive neighborhoods are NridgHt, NoRidge and StoneBr with median house prices of approximately $300,000, three times as high as the median of the 3 cheapest neighborhoods, which are BrDale, DOTRR and MeadowV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "visualizeCatFeature('Neighborhood')\n",
    "title = plt.title('House Price by Neighborhood')\n",
    "tick = plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roof Material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Houses using Wood Shingles roof are the most expensive with price ranging from $300,000 to \\$450,000. There are also a lot of expensive houses using Standard Composite Shingle roof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "visualizeCatFeature('RoofMatl')\n",
    "title = plt.title('House Price by Roof Material')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kitchen Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kitchen Quality is another important feature to predict house price. There is a very big difference in price between houses with different kitchen quality. For example, the average price difference between a house with a good kitchen and one with an excellent kitchen is about $120,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "visualizeCatFeature('KitchenQual')\n",
    "title = plt.title('House Price by Kitchen Quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4. Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix from train data excluding `SalePrice`\n",
    "corr_mat = train_data.iloc[:, :-1].corr()\n",
    "\n",
    "# Select correlations greater than 0.5\n",
    "high_corr_mat = corr_mat[abs(corr_mat) >= 0.5]\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(high_corr_mat,\n",
    "            annot=True,\n",
    "            fmt='.1f',\n",
    "            cmap='GnBu',\n",
    "            vmin=0.5,\n",
    "            vmax=1)\n",
    "title = plt.title('Correlation Heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is multicollinearity in our training data. Below features are highly correlated:\n",
    "- GarageCars and GarageArea\n",
    "- GarageYrBlt and YearBuilt\n",
    "- 1stFlrSF and TotalBsmtSF\n",
    "- GrLivArea and TotRmsAbvGrd\n",
    "\n",
    "Multicolliniearity has a negative impact on our prediction models and makes standard errors of our estimates increase. Therefore, for each pair of highly correlated features, I will remove a feature that has a lower correlation with SalePrice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5. Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms give an error when we train them on data with missing values. Therefore, it's important to identify them before deciding how to handle them (drop features or impute missing value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_count = X.isnull().sum()\n",
    "missing_data_percent = X.isnull().sum() / len(X) * 100\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Count': missing_data_count,\n",
    "    'Percent': missing_data_percent\n",
    "})\n",
    "missing_data = missing_data[missing_data.Count > 0]\n",
    "missing_data.sort_values(by='Count', ascending=False, inplace=True)\n",
    "\n",
    "print(f\"There are {missing_data.shape[0]} features having missing data.\\n\")\n",
    "print(\"Top 10 missing value features:\")\n",
    "missing_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(y=missing_data.head(18).index,\n",
    "            x=missing_data.head(18).Count,\n",
    "            palette='GnBu_r')\n",
    "title = plt.title(\"Missing Values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some basic understandings of the data set and features, let's move to data preprocessing and modeling steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_count = X.isnull().sum()\n",
    "missing_data_percent = X.isnull().sum() / len(X) * 100\n",
    "missing_data = pd.DataFrame({\n",
    "    'Count': missing_data_count,\n",
    "    'Percent': missing_data_percent\n",
    "})\n",
    "missing_data = missing_data[missing_data.Count > 0]\n",
    "missing_data.sort_values(by='Count', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# Group 1:\n",
    "group_1 = [\n",
    "    'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType',\n",
    "    'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n",
    "    'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'\n",
    "]\n",
    "X[group_1] = X[group_1].fillna(\"None\")\n",
    "\n",
    "# Group 2:\n",
    "group_2 = [\n",
    "    'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
    "    'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'\n",
    "]\n",
    "\n",
    "X[group_2] = X[group_2].fillna(0)\n",
    "\n",
    "# Group 3:\n",
    "group_3a = [\n",
    "    'Functional', 'MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st',\n",
    "    'Exterior2nd', 'SaleType', 'Utilities'\n",
    "]\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X[group_3a] = pd.DataFrame(imputer.fit_transform(X[group_3a]), index=X.index)\n",
    "\n",
    "X.LotFrontage = X.LotFrontage.fillna(X.LotFrontage.mean())\n",
    "X.GarageYrBlt = X.GarageYrBlt.fillna(X.YearBuilt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether there is any missing value left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(X.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All missing values have been handled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because regression models are very sensitive to outlier, we need to be aware of them. Let's examine outliers with a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='GrLivArea', y='SalePrice', data=train_data)\n",
    "title = plt.title('House Price vs. Living Area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two observations lying separately from the rest. They have large living area but low price. They are the outliers that we are looking for. I will delete them from the training set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_index = train_data[(train_data.GrLivArea > 4000)\n",
    "                           & (train_data.SalePrice < 200000)].index\n",
    "X.drop(outlier_index, axis=0, inplace=True)\n",
    "y.drop(outlier_index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3.2. Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinal categorical features are label encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Ordinal categorical columns\n",
    "label_encoding_cols = [\n",
    "    \"Alley\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\",\n",
    "    \"BsmtQual\", \"ExterCond\", \"ExterQual\", \"FireplaceQu\", \"Functional\",\n",
    "    \"GarageCond\", \"GarageQual\", \"HeatingQC\", \"KitchenQual\", \"LandSlope\",\n",
    "    \"LotShape\", \"PavedDrive\", \"PoolQC\", \"Street\", \"Utilities\"\n",
    "]\n",
    "\n",
    "# Apply Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for col in label_encoding_cols:\n",
    "    X[col] = label_encoder.fit_transform(X[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3.3. Transform Numerical Variables to Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Because I have calculated age of houses, YearBuilt is no longer needed. However, YrSold could have a large impact on house price (e.g. In economic crisis years, house price could be lower). Therefore, I will transform it into categorical variables.\n",
    "Like YrSold, some numerical variables don't have any ordinal meaning (e.g. MoSold, MSSubClass). I will transform them into categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_factor_cols = ['YrSold', 'MoSold', 'MSSubClass']\n",
    "\n",
    "for col in to_factor_cols:\n",
    "    X[col] = X[col].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4. Skewness and Normalizing Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal distribution is one of the assumption that linear regression relies on. Therefore, transfoming skewed data will help our models perform better.\n",
    "First, let's examine the target variable SalePrice with Distribution plot and Quantile-Quantile plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normality_plot(X):\n",
    "    \"\"\"\n",
    "    1. Draw distribution plot with normal distribution fitted curve\n",
    "    2. Draw Quantile-Quantile plot \n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    sns.distplot(X, fit=norm, ax=axes[0])\n",
    "    axes[0].set_title('Distribution Plot')\n",
    "\n",
    "    axes[1] = stats.probplot((X), plot=plt)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_plot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the methods to normalize right-skewed data is using log transformation because big values will be pulled to the center. However, log(0) is Nan, so I will use log(1+X) to fix skewness instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(1 + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is SalePrice after log transformation. The sknewness has been fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_plot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step I will examine skewness in the rest of numerical variables and use log transformation to fix them,\n",
    "\n",
    "Fixing skewness in other numerical variables\n",
    "\n",
    "If skewness is less than -1 or greater than 1, the distribution is highly skewed.\n",
    "\n",
    "If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\n",
    "\n",
    "If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\n",
    "Below are skewed features in our original train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = train_data.skew().sort_values(ascending=False)\n",
    "skewness[abs(skewness) > 0.75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check normality of GrLivArea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_plot(X.GrLivArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of skewed columns\n",
    "skewed_cols = list(skewness[abs(skewness) > 0.5].index)\n",
    "\n",
    "# Remove 'MSSubClass' and 'SalePrice'\n",
    "skewed_cols = [\n",
    "    col for col in skewed_cols if col not in ['MSSubClass', 'SalePrice']\n",
    "]\n",
    "\n",
    "# Log-transform skewed columns\n",
    "for col in skewed_cols:\n",
    "    X[col] = np.log(1 + X[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is normality of GrLivArea after log-transformation. Skewness has been fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_plot(X.GrLivArea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5. Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for Decision Tree and Random Forest, it is highly recommended to standardize the data set before running machine learning algorithms since optimization methods and gradient descent run and converge faster on similarly scaled features.\n",
    "\n",
    "\n",
    "However, outliers can often influence the sample mean and standard deviation in a negative way, and models like Lasso and Elastic Net are very sensitive to outliers. In such cases, the median and the interquartile range often give better results. I will use RobustScaler to transform the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "numerical_cols = list(X.select_dtypes(exclude=['object']).columns)\n",
    "scaler = RobustScaler()\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6. One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, drop_first=True)\n",
    "print(\"X.shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing the train and test data, I split them again to perform modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntest = len(test_data)\n",
    "X_train = X.iloc[:-ntest, :]\n",
    "X_test = X.iloc[-ntest:, :]\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In model evaluation, it's a common practice to split the entire training data into 2 sets of data (train and test). However, a model may work very well on a set of test data but have a poor performance on other sets of unseen data.\n",
    "A solution to this problem is a procedure called cross-validation (CV). In the example below, under the basic approach, called k-fold CV, the training set is split into 5 smaller sets. Then, for each fold, a model is trained using the other 4 folds and evaluated on the remaining fold. The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will write a function to get the Root Mean Squared Logarithmic Error (RMSLE) for my models using cross-validation. There is one note here: because I have transformed the target variable to log(1+y) , the Mean Squared Error for log(1+y) is the Mean Squared Logarithmic Error for SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "\n",
    "def getRMSLE(model):\n",
    "    \"\"\"\n",
    "    Return the average RMSLE over all folds of training data.\n",
    "    \"\"\"\n",
    "    # Set KFold to shuffle data before the split\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Get RMSLE score\n",
    "    rmse = np.sqrt(-cross_val_score(\n",
    "        model, X_train, y, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "\n",
    "    return rmse.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1. Regularized Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.1. Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regularized linear regression (Ridge), we try to minimize:\n",
    "𝐽(𝜃)=12𝑚(∑𝑖=1𝑚(ℎ𝜃(𝑥(𝑖))−𝑦(𝑖))2)+𝜆2𝑚(∑𝑗=1𝑛𝜃2𝑗)\n",
    "J\n",
    "(\n",
    "θ\n",
    ")\n",
    "=\n",
    "1\n",
    "2\n",
    "m\n",
    "(\n",
    "∑\n",
    "i\n",
    "=\n",
    "1\n",
    "m\n",
    "(\n",
    "h\n",
    "θ\n",
    "(\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "−\n",
    "y\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "2\n",
    ")\n",
    "+\n",
    "λ\n",
    "2\n",
    "m\n",
    "(\n",
    "∑\n",
    "j\n",
    "=\n",
    "1\n",
    "n\n",
    "θ\n",
    "j\n",
    "2\n",
    ")\n",
    " \n",
    "where  𝜆\n",
    "λ\n",
    "  is a regularization parameter which controls the degree of regularization (thus, help preventing overfitting). The regularization term puts a penalty on the overall cost J. As the magnitudes of the model parameters  𝜃𝑗\n",
    "θ\n",
    "j\n",
    "  increase, the penalty increases as well.\n",
    "I will find the  𝜆\n",
    "λ\n",
    "  that gives me the smallest RMSLE from cross-validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_list = list(np.linspace(20, 25, 101))\n",
    "\n",
    "rmsle_ridge = [getRMSLE(Ridge(alpha=lambda_)) for lambda_ in lambda_list]\n",
    "rmsle_ridge = pd.Series(rmsle_ridge, index=lambda_list)\n",
    "\n",
    "rmsle_ridge.plot(title=\"RMSLE by lambda\")\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.ylabel(\"RMSLE\")\n",
    "\n",
    "print(\"Best lambda:\", rmsle_ridge.idxmin())\n",
    "print(\"RMSLE:\", rmsle_ridge.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=22.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.2. Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression is very similar to Ridge regression. One difference is that in the regularization term, instead of using sum of squared of  𝜃\n",
    "θ\n",
    " , we use sum of absolute value of  𝜃\n",
    "θ\n",
    " :\n",
    "𝐽(𝜃)=12𝑚(∑𝑖=1𝑚(ℎ𝜃(𝑥(𝑖))−𝑦(𝑖))2)+𝜆2𝑚(∑𝑗=1𝑛|𝜃𝑗|)\n",
    "J\n",
    "(\n",
    "θ\n",
    ")\n",
    "=\n",
    "1\n",
    "2\n",
    "m\n",
    "(\n",
    "∑\n",
    "i\n",
    "=\n",
    "1\n",
    "m\n",
    "(\n",
    "h\n",
    "θ\n",
    "(\n",
    "x\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "−\n",
    "y\n",
    "(\n",
    "i\n",
    ")\n",
    ")\n",
    "2\n",
    ")\n",
    "+\n",
    "λ\n",
    "2\n",
    "m\n",
    "(\n",
    "∑\n",
    "j\n",
    "=\n",
    "1\n",
    "n\n",
    "|\n",
    "θ\n",
    "j\n",
    "|\n",
    ")\n",
    " \n",
    "Another big difference is that Ridge Regresion can only shrink parameters close to zero while Lasso Regression can shrink some parameters all the way to 0. Therefore, we can use Lasso Regression to perform feature selection and regression.\n",
    "With the same method above, the best lambda_ for my Lasso model is 0.00065."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_list = list(np.linspace(0.0006, 0.0007, 11))\n",
    "rmsle_lasso = [\n",
    "    getRMSLE(Lasso(alpha=lambda_, max_iter=100000)) for lambda_ in lambda_list\n",
    "]\n",
    "rmsle_lasso = pd.Series(rmsle_lasso, index=lambda_list)\n",
    "\n",
    "rmsle_lasso.plot(title=\"RMSLE by lambda\")\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.ylabel(\"RMSLE\")\n",
    "\n",
    "print(\"Best lambda:\", rmsle_lasso.idxmin())\n",
    "print(\"RMSLE:\", rmsle_lasso.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.00065, max_iter=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this complete guide of parameter tuning for XGBoost, I respectively tune and find the best parameter for  n_estimators max_depth min_child_weight gamma subsample colsample_bytree reg_alpha reg_lambda learning_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(learning_rate=0.05,\n",
    "                   n_estimators=2100,\n",
    "                   max_depth=2,\n",
    "                   min_child_weight=2,\n",
    "                   gamma=0,\n",
    "                   subsample=0.65,\n",
    "                   colsample_bytree=0.46,\n",
    "                   nthread=-1,\n",
    "                   scale_pos_weight=1,\n",
    "                   reg_alpha=0.464,\n",
    "                   reg_lambda=0.8571,\n",
    "                   silent=1,\n",
    "                   random_state=7,\n",
    "                   n_jobs=2)\n",
    "getRMSLE(xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LightGBM is a powerful gradient boosting framework based on decision tree algorithm. Like XGBoost, LightGBM has a high performance on large data sets but much faster training speed than XGBoost does. Following this guide, I have tuned the parameters num_leaves min_data_in_leaf max_depth bagging_fraction feature_fraction max_bin. As you can see in the RMSLE reported below, for this data set LightGBM has better performance than XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = LGBMRegressor(objective='regression',\n",
    "                    learning_rate=0.05,\n",
    "                    n_estimators=730,\n",
    "                    num_leaves=8,\n",
    "                    min_data_in_leaf=4,\n",
    "                    max_depth=3,\n",
    "                    max_bin=55,\n",
    "                    bagging_fraction=0.78,\n",
    "                    bagging_freq=5,\n",
    "                    feature_fraction=0.24,\n",
    "                    feature_fraction_seed=9,\n",
    "                    bagging_seed=9,\n",
    "                    min_sum_hessian_in_leaf=11)\n",
    "getRMSLE(lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4. Averaging Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularized regression and gradient boosting work very differently and they may perform well on different data points. Thus it is a good practice to get average predictions from these models. Below I create a new class for my averaging model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin, clone\n",
    "\n",
    "\n",
    "class AveragingModel(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Create clone models\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "\n",
    "        # Train cloned models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Get predictions from trained clone models\n",
    "        predictions = np.column_stack(\n",
    "            [model.predict(X) for model in self.models_])\n",
    "\n",
    "        # Return average predictions\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_model = AveragingModel(models=(ridge, lasso, xgb, lgb))\n",
    "getRMSLE(avg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSLE score of the averaging model is much better than any of base models. I will use this model as my final model. In the last step, I will train my final model on the whole training data, make predictions from the test data and save my output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = avg_model\n",
    "my_model.fit(X_train, y)\n",
    "predictions = my_model.predict(X_test)\n",
    "final_predictions = np.exp(predictions) - 1\n",
    "output = pd.DataFrame({'Id': test_data.index, 'SalePrice': final_predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In this project, I have conducted a detailed EDA to understand the data and important features. Based on exploratory analysis, I performed data preprocessing and feature engineering. Finally, I train regularized regression models (Ridge, Lasso), XGBoost and LightGBM, and take average predictions from these models to predict final price of each house. By the time I write this notebook, my best model has Mean Absolute Error of 12293.919, ranking 95/15502, approximately top 0.6% in the Kaggle leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
