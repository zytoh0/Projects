{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Reddit - Preprocessing & Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Notebook 4\n",
    "\n",
    "The fourth notebook will examine preprocessing techniques such as lemmatization, stemming and adding more stopwords and optimising models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Package and Read in Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize, WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Import Evaluations\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>emoji</th>\n",
       "      <th>website</th>\n",
       "      <th>word_count</th>\n",
       "      <th>title_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i came home sick from work with pneumonia here...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>my sleepyhead curled up this morning</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>spooky takes care of me when i wfh who needs h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>hi i am a new cat mom and am hoping someone mi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>spooky looks after me when i wfh who needs a h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                              title  emoji  \\\n",
       "0          0  i came home sick from work with pneumonia here...      0   \n",
       "1          0               my sleepyhead curled up this morning      0   \n",
       "2          0  spooky takes care of me when i wfh who needs h...      0   \n",
       "3          0  hi i am a new cat mom and am hoping someone mi...      0   \n",
       "4          0  spooky looks after me when i wfh who needs a h...      0   \n",
       "\n",
       "   website  word_count  title_length  \n",
       "0        0          23           121  \n",
       "1        0           6            36  \n",
       "2        0          15            67  \n",
       "3        0          25           128  \n",
       "4        0          19            94  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the dataset\n",
    "df_reddit = pd.read_csv('./dataset/cleaned.csv') \n",
    "df_reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19311, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of rows and columns\n",
    "df_reddit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text to Matrix Representation\n",
    "We will use countvectorise to split the words of the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19311, 13472)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_reddit['title']\n",
    "\n",
    "# Instantiate a CountVectorizer with english stopwords.\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit the vectorizer on our corpus and transform it.\n",
    "X_vec = cvec.fit_transform(X)\n",
    "\n",
    "# Create a dataframe after count vectoriser\n",
    "df_vec_reddit = pd.DataFrame(X_vec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "df_vec_reddit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the title into words, the number of columns change from 1 to 13472. \n",
    "\n",
    "We will try to reduce the number of columns using:\n",
    "* Increasing the stopwords\n",
    "* Stemming \n",
    "* Lemmatization\n",
    "for the model to perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Stopwords\n",
    "To find and add more stopwords, we will\n",
    "* Check if the vectorised columns and the original columns are the same.\n",
    "* Drop the vectorised columns with the same name as the original.\n",
    "* Add back the original columns\n",
    "* Split the dataframe into dog and cat subreddit\n",
    "* Print out the top words for each subreddit\n",
    "* Add more stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19311, 13470)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the two columns as we want to add back the original columns\n",
    "df_vec_reddit.drop(columns =['subreddit', 'website'], inplace = True)\n",
    "\n",
    "df_vec_reddit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19311, 13474)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding back the columns after countvectoriser\n",
    "df_vec_reddit = pd.concat([df_vec_reddit, df_reddit[['subreddit', 'emoji', 'word_count', 'title_length']]], axis=1)\n",
    "\n",
    "# Checking if 5 rows have been added.\n",
    "df_vec_reddit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the dataframe into cats and dogs to find the top words for each subreddit\n",
    "df_vec_reddit_dogs = df_vec_reddit[df_vec_reddit['subreddit'] == 1]\n",
    "df_vec_reddit_cats = df_vec_reddit[df_vec_reddit['subreddit'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title_length       453198\n",
       "word_count          87343\n",
       "subreddit            9737\n",
       "dog                  4961\n",
       "dogs                 1257\n",
       "help                  805\n",
       "puppy                 579\n",
       "old                   402\n",
       "advice                362\n",
       "need                  312\n",
       "does                  302\n",
       "breed                 293\n",
       "new                   291\n",
       "food                  283\n",
       "just                  227\n",
       "im                    223\n",
       "like                  215\n",
       "know                  197\n",
       "best                  190\n",
       "getting               183\n",
       "training              175\n",
       "time                  170\n",
       "wont                  165\n",
       "year                  164\n",
       "stop                  156\n",
       "dont                  156\n",
       "home                  156\n",
       "looking               155\n",
       "good                  149\n",
       "vet                   138\n",
       "ate                   137\n",
       "pet                   135\n",
       "got                   128\n",
       "breeds                122\n",
       "rescue                119\n",
       "question              116\n",
       "owner                 115\n",
       "today                 114\n",
       "tips                  111\n",
       "month                 110\n",
       "small                 109\n",
       "eat                   107\n",
       "think                 105\n",
       "recommendations       105\n",
       "aggressive            105\n",
       "want                  105\n",
       "people                105\n",
       "bad                   104\n",
       "house                 100\n",
       "pup                    99\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vec_reddit_dogs.sum().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title_length    497550\n",
       "word_count       99737\n",
       "cat               2726\n",
       "emoji             1008\n",
       "cats               707\n",
       "new                580\n",
       "just               492\n",
       "like               419\n",
       "little             400\n",
       "year               354\n",
       "kitten             327\n",
       "im                 298\n",
       "old                287\n",
       "got                282\n",
       "love               275\n",
       "kitty              272\n",
       "hes                261\n",
       "help               260\n",
       "boy                254\n",
       "shes               253\n",
       "happy              253\n",
       "years              235\n",
       "does               231\n",
       "baby               222\n",
       "time               218\n",
       "day                199\n",
       "loves              198\n",
       "know               186\n",
       "think              183\n",
       "best               181\n",
       "today              178\n",
       "cute               176\n",
       "home               174\n",
       "meet               174\n",
       "dont               168\n",
       "christmas          157\n",
       "ago                157\n",
       "good               155\n",
       "months             146\n",
       "look               143\n",
       "adopted            137\n",
       "girl               136\n",
       "night              134\n",
       "need               129\n",
       "sweet              126\n",
       "sleeping           124\n",
       "bed                119\n",
       "food               118\n",
       "advice             118\n",
       "big                112\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vec_reddit_cats.sum().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original list of english stop words in SkLearn Library\n",
    "text.ENGLISH_STOP_WORDS\n",
    "\n",
    "# Create list for new stop words\n",
    "add_stop_words = ['im', 'does']\n",
    "\n",
    "# Joining new list of stop words to list in SKLearn Library\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Print to check\n",
    "#stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lemmatizer.\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create function to lemmatize\n",
    "def lemmatize_text(text):\n",
    "    sentence = \" \".join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])\n",
    "    return sentence\n",
    "\n",
    "df_lemmatized = pd.DataFrame(df_reddit['title'].apply(lemmatize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19311, 12196)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_lemmatized['title']\n",
    "\n",
    "# Instantiate a CountVectorizer with the default hyperparameters.\n",
    "cvec = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fit the vectorizer on our corpus and transform it.\n",
    "X_vec = cvec.fit_transform(X)\n",
    "\n",
    "# Create a dataframe after count vectoriser\n",
    "df_lemma_reddit = pd.DataFrame(X_vec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "df_lemma_reddit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization reduces the number of columns from 13472 to 12196. A total of 1276 words were reduced to similar forms as the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i came home sick from work with pneumonia here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my sleepyhead curl up thi morn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spooki take care of me when i wfh who need hr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi i am a new cat mom and am hope someon might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spooki look after me when i wfh who need a hr ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  i came home sick from work with pneumonia here...\n",
       "1                     my sleepyhead curl up thi morn\n",
       "2  spooki take care of me when i wfh who need hr ...\n",
       "3  hi i am a new cat mom and am hope someon might...\n",
       "4  spooki look after me when i wfh who need a hr ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Create function to stem\n",
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "df_stemming = pd.DataFrame(df_reddit['title'].apply(stem_sentences))\n",
    "\n",
    "df_stemming.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19311, 10178)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_stemming['title']\n",
    "\n",
    "# Instantiate a CountVectorizer with the default hyperparameters.\n",
    "cvec = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fit the vectorizer on our corpus and transform it.\n",
    "X_vec = cvec.fit_transform(X)\n",
    "\n",
    "# Create a dataframe after count vectoriser\n",
    "df_stem_reddit = pd.DataFrame(X_vec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "df_stem_reddit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming reduces the number of columns from 13472 to 10178. A total of 3294 words were reduced to similar forms as the rest. Stemming is the better preprocessing choice as it helps reduce more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19311, 454)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reloading the data and implement what we found above.\n",
    "# Stemming\n",
    "df_model = pd.read_csv('./dataset/cleaned.csv') \n",
    "df_model['title'] = pd.DataFrame(df_model['title'].apply(stem_sentences))\n",
    "\n",
    "# Count Vectoriser with the additional stop words\n",
    "X = df_model['title']\n",
    "\n",
    "# Instantiate a CountVectorizer with the new added stop words and min df = 3 and ngram range from 1 to 3.\n",
    "cvec = CountVectorizer(stop_words = stop_words, min_df = 3, ngram_range=(1,3), max_features = 450)\n",
    "\n",
    "# Fit the vectorizer on our corpus and transform it.\n",
    "X_vec = cvec.fit_transform(X)\n",
    "\n",
    "# Create a dataframe after count vectoriser\n",
    "df_vec_model = pd.DataFrame(X_vec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "# Remove the subreddit columns and add back the original columns\n",
    "#df_vec_model.drop(columns =['subreddit'], inplace = True)\n",
    "df_vec_model = pd.concat([df_vec_model, df_model[['subreddit', 'emoji', 'word_count', 'title_length']]], axis=1)\n",
    "\n",
    "df_vec_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable\n",
    "y = df_vec_model.pop('subreddit')\n",
    "X = df_vec_model\n",
    "\n",
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Modelling Mass Fitting\n",
    "We will first attempt to fit a variety of models and pick the top 2 for optimatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "etc = ExtraTreesClassifier(n_jobs=-1)\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "nb = MultinomialNB()\n",
    "ada = AdaBoostClassifier()\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "bag = BaggingClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to find the train and test accuracy scores of the models with 5 fold cross validation\n",
    "def report_error(model, X1, y1, X2, y2):\n",
    "    model.fit(X1, y1)\n",
    "    print('The accuracy train score for', model, 'is', cross_val_score(model, X1, y1, cv=5).mean(),'.')\n",
    "    print('The accuracy test score for',model, 'is', cross_val_score(model, X2, y2, cv=5).mean(),'.')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy train score for RandomForestClassifier(n_jobs=-1) is 0.8940316249013884 .\n",
      "The accuracy test score for RandomForestClassifier(n_jobs=-1) is 0.8806604374317143 .\n",
      "\n",
      "The accuracy train score for ExtraTreesClassifier(n_jobs=-1) is 0.89435510282336 .\n",
      "The accuracy test score for ExtraTreesClassifier(n_jobs=-1) is 0.8819527579111061 .\n",
      "\n",
      "The accuracy train score for LogisticRegression(n_jobs=-1) is 0.9044535521701915 .\n",
      "The accuracy test score for LogisticRegression(n_jobs=-1) is 0.8923064032871058 .\n",
      "\n",
      "The accuracy train score for MultinomialNB() is 0.8990157789253234 .\n",
      "The accuracy test score for MultinomialNB() is 0.8938651643217662 .\n",
      "\n",
      "The accuracy train score for AdaBoostClassifier() is 0.8781069480283415 .\n",
      "The accuracy test score for AdaBoostClassifier() is 0.865903652414052 .\n",
      "\n",
      "The accuracy train score for KNeighborsClassifier(n_jobs=-1) is 0.7863156769872426 .\n",
      "The accuracy test score for KNeighborsClassifier(n_jobs=-1) is 0.7501886868334797 .\n",
      "\n",
      "The accuracy train score for BaggingClassifier(n_jobs=-1) is 0.8768779288864025 .\n",
      "The accuracy test score for BaggingClassifier(n_jobs=-1) is 0.8534791439047116 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_error(rfc, X_train, y_train, X_test, y_test)\n",
    "report_error(etc, X_train, y_train, X_test, y_test)\n",
    "report_error(lr, X_train, y_train, X_test, y_test)\n",
    "report_error(nb, X_train, y_train, X_test, y_test)\n",
    "report_error(ada, X_train, y_train, X_test, y_test)\n",
    "report_error(knn, X_train, y_train, X_test, y_test)\n",
    "report_error(bag, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models does not show signs of overfitting. All models except K-Nearest Neigbours have accuracy scores above 85%. The top 2 performing models are LogisticRegression and MultinomialNB with MultinomialNB scoring about 0.01 higher. We will proceed to optimise using grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Model Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the data and implement what we found above.\n",
    "# Stemming\n",
    "df_model = pd.read_csv('./dataset/cleaned.csv') \n",
    "df_model['title'] = pd.DataFrame(df_model['title'].apply(stem_sentences))\n",
    "\n",
    "# Define target variable\n",
    "y = df_model.pop('subreddit')\n",
    "X = df_model['title']\n",
    "\n",
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9005928721286421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.35,\n",
       " 'cvec__max_features': 3500,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up pipeline\n",
    "pipe_nb_c = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words)),\n",
    "    ('nb', MultinomialNB())])\n",
    "\n",
    "# Set up pipeline params\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [3000, 3500, 4000],\n",
    "    'cvec__min_df': [1, 2, 3],\n",
    "    'cvec__max_df': [0.25, 0.35, 0.45],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3)]\n",
    "}\n",
    "\n",
    "# Set up a gridsearch\n",
    "gs_nb_c = GridSearchCV(pipe_nb_c, param_grid = pipe_params, cv=5)\n",
    "\n",
    "# Fit the gridsearch\n",
    "gs_nb_c.fit(X_test, y_test)\n",
    "\n",
    "# Find best score\n",
    "print(gs_nb_c.best_score_)\n",
    "\n",
    "# Find best parameters\n",
    "gs_nb_c.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9047325875232088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.3,\n",
       " 'cvec__max_features': 1500,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up pipeline\n",
    "pipe_lr_c = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words)),\n",
    "    ('lr', LogisticRegression())])\n",
    "\n",
    "# Set up pipeline params\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [1000, 1500, 2000],\n",
    "    'cvec__min_df': [1, 2, 3],\n",
    "    'cvec__max_df': [0.25, 0.3, 0.35],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3)]\n",
    "}\n",
    "\n",
    "# Set up a gridsearch\n",
    "gs_lr_c = GridSearchCV(pipe_lr_c, param_grid = pipe_params, cv=5)\n",
    "\n",
    "# Fit the gridsearch\n",
    "gs_lr_c.fit(X_test, y_test)\n",
    "\n",
    "# Find best score\n",
    "print(gs_lr_c.best_score_)\n",
    "\n",
    "# Find best parameters\n",
    "gs_lr_c.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Choice of Model - MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy train score for MultinomialNB() is 0.9228375873885936 .\n",
      "The accuracy test score for MultinomialNB() is 0.9112072605889174 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reloading the data and implement what we found above.\n",
    "# Stemming\n",
    "df_model = pd.read_csv('./dataset/cleaned.csv') \n",
    "df_model['title'] = pd.DataFrame(df_model['title'].apply(stem_sentences))\n",
    "\n",
    "# Count Vectoriser with the additional stop words\n",
    "X = df_model['title']\n",
    "\n",
    "# Instantiate a CountVectorizer with the new added stop words and min df = 3 and ngram range from 1 to 3.\n",
    "cvec = CountVectorizer(stop_words = stop_words, min_df = 1, max_df = 0.35,  ngram_range=(1,1), max_features = 3500)\n",
    "\n",
    "# Fit the vectorizer on our corpus and transform it.\n",
    "X_vec = cvec.fit_transform(X)\n",
    "\n",
    "# Create a dataframe after count vectoriser\n",
    "df_vec_model = pd.DataFrame(X_vec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "# Remove the subreddit columns and add back the original columns\n",
    "df_vec_model.drop(columns =['subreddit'], inplace = True)\n",
    "df_vec_model = pd.concat([df_vec_model, df_model[['subreddit', 'emoji', 'word_count', 'title_length']]], axis=1)\n",
    "\n",
    "# Define target variable\n",
    "y = df_vec_model.pop('subreddit')\n",
    "X = df_vec_model\n",
    "\n",
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "report_error(nb, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Choice of Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy train score for LogisticRegression(n_jobs=-1) is 0.8539612425759637 .\n",
      "The accuracy test score for LogisticRegression(n_jobs=-1) is 0.8350967564632782 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reloading the data and implement what we found above.\n",
    "# Stemming\n",
    "df_model = pd.read_csv('./dataset/cleaned.csv') \n",
    "df_model['title'] = pd.DataFrame(df_model['title'].apply(stem_sentences))\n",
    "\n",
    "# Count Vectoriser with the additional stop words\n",
    "X = df_model['title']\n",
    "\n",
    "# Instantiate a CountVectorizer with the new added stop words and min df = 3 and ngram range from 1 to 3.\n",
    "cvec = CountVectorizer(stop_words = stop_words, min_df = 2, max_df = 0.3,  ngram_range=(1,1), max_features = 1500)\n",
    "\n",
    "# Fit the vectorizer on our corpus and transform it.\n",
    "X_vec = cvec.fit_transform(X)\n",
    "\n",
    "# Create a dataframe after count vectoriser\n",
    "df_vec_model = pd.DataFrame(X_vec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "# Remove the subreddit columns and add back the original columns\n",
    "df_vec_model.drop(columns =['subreddit'], inplace = True)\n",
    "df_vec_model = pd.concat([df_vec_model, df_model[['subreddit', 'emoji', 'word_count', 'title_length']]], axis=1)\n",
    "\n",
    "# Define target variable\n",
    "y = df_vec_model.pop('subreddit')\n",
    "X = df_vec_model\n",
    "\n",
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "report_error(lr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the top 2 models, logistic regression and Naive Bayes after optimisation, the 2 models both score 0.90 with logistic regression been slightly higher. After adding in the held out features such as emoji, word_count and title_length, the Navie Bayes model improve to 0.91 while logistic regression dropped to 0.83. \n",
    "\n",
    "The withheld features were quite significant to the model training. As discussed previously, cat owners like to post a lot of emojis while dog owners tend to write a lot and have higher word_count and title_length.\n",
    "\n",
    "Thus our choice of model would be Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model's Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes is a classification method based on Bayes’ theorem that derives the probability of the given feature vector being associated with a label. Naïve Bayes has a naive assumption of conditional independence for every feature, which means that the algorithm expects the features to be independent which not always is the case.\n",
    "\n",
    "Logistic regression is a linear classification method that learns the probability of a sample belonging to a certain class. Logistic regression tries to find the optimal decision boundary that best separates the classes.\n",
    "\n",
    "How the 2 models work is while Naive Bayes tries to generalise the features to the output, logistic regresssion tries to disciminate them instead. While both models performs equally during the NLP process, logistic regression performance dropped when additional withheld features are added back. It fails to recognise that the withheld features belongs to which class. Naive Bayes on the other hand improves its performance by grouping the withheld features with the current ones and better be able to classify all the features with the output variable."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
