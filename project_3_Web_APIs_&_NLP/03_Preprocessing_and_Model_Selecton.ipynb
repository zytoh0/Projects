{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Notebook 3\n",
    "\n",
    "The third notebook will examine preprocessing techniques and try out a few models.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Package and Read in Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize, WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Import Evaluations\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dogs</td>\n",
       "      <td>My dog bites himself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dogs</td>\n",
       "      <td>How do you keep your dogs passively stimulated?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dogs</td>\n",
       "      <td>Dog's Sleeping Behavior Changed with Pregnant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>dogs</td>\n",
       "      <td>Soft food diet after surgery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>dogs</td>\n",
       "      <td>Does anyone know much about Intracranial Arach...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 subreddit                                              title\n",
       "0           0      dogs                               My dog bites himself\n",
       "1           1      dogs    How do you keep your dogs passively stimulated?\n",
       "2           2      dogs  Dog's Sleeping Behavior Changed with Pregnant ...\n",
       "3           3      dogs                       Soft food diet after surgery\n",
       "4           4      dogs  Does anyone know much about Intracranial Arach..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the dataset\n",
    "df_reddit = pd.read_csv('./dataset/cleaned.csv') \n",
    "df_reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19993, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of rows and columns\n",
    "df_reddit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text to Matrix Representation\n",
    "We will use countvectorise to split the words of the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19993, 12974)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_reddit['title']\n",
    "\n",
    "# Instantiate a CountVectorizer with english stopwords.\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit the vectorizer on our corpus and transform it.\n",
    "X_vec = cvec.fit_transform(X)\n",
    "\n",
    "# Create a dataframe after count vectoriser\n",
    "df_vec_reddit = pd.DataFrame(X_vec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "df_vec_reddit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the title into words, the number of columns change from 1 to 13472. \n",
    "\n",
    "We will try to reduce the number of columns using:\n",
    "* Increasing the stopwords\n",
    "* Stemming \n",
    "* Lemmatization\n",
    "for the model to perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Stopwords\n",
    "To find and add more stopwords, we will\n",
    "* Check if the vectorised columns and the original columns are the same.\n",
    "* Drop the vectorised columns with the same name as the original.\n",
    "* Add back the original columns\n",
    "* Split the dataframe into dog and cat subreddit\n",
    "* Print out the top words for each subreddit\n",
    "* Add more stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19993, 12972)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the two columns as we want to add back the original columns\n",
    "df_vec_reddit.drop(columns =['subreddit', 'website'], inplace = True)\n",
    "\n",
    "df_vec_reddit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding back the columns after countvectoriser\n",
    "df_vec_reddit = pd.concat([df_vec_reddit, df_reddit[['subreddit', 'emoji', 'word_count', 'title_length']]], axis=1)\n",
    "\n",
    "# Checking if 5 rows have been added.\n",
    "df_vec_reddit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the dataframe into cats and dogs to find the top words for each subreddit\n",
    "df_vec_reddit_dogs = df_vec_reddit[df_vec_reddit['subreddit'] == 1]\n",
    "df_vec_reddit_cats = df_vec_reddit[df_vec_reddit['subreddit'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_vec_reddit_dogs.sum().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_vec_reddit_cats.sum().sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original list of english stop words in SkLearn Library\n",
    "text.ENGLISH_STOP_WORDS\n",
    "\n",
    "# Create list for new stop words\n",
    "add_stop_words = ['im', 'does']\n",
    "\n",
    "# Joining new list of stop words to list in SKLearn Library\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Print to check\n",
    "#stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lemmatizer.\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create function to lemmatize\n",
    "def lemmatize_text(text):\n",
    "    sentence = \" \".join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])\n",
    "    return sentence\n",
    "\n",
    "df_lemmatized = pd.DataFrame(df_reddit['title'].apply(lemmatize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19278, 12217)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_lemmatized['title']\n",
    "\n",
    "# Instantiate a CountVectorizer with the default hyperparameters.\n",
    "cvec = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fit the vectorizer on our corpus and transform it.\n",
    "X_vec = cvec.fit_transform(X)\n",
    "\n",
    "# Create a dataframe after count vectoriser\n",
    "df_lemma_reddit = pd.DataFrame(X_vec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "df_lemma_reddit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization reduces the number of columns from 13472 to 12196. A total of 1276 words were reduced to similar forms as the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i came home sick from work with pneumonia here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my sleepyhead curl up thi morn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spooki take care of me when i wfh who need hr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi i am a new cat mom and am hope someon might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spooki look after me when i wfh who need a hr ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  i came home sick from work with pneumonia here...\n",
       "1                     my sleepyhead curl up thi morn\n",
       "2  spooki take care of me when i wfh who need hr ...\n",
       "3  hi i am a new cat mom and am hope someon might...\n",
       "4  spooki look after me when i wfh who need a hr ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Create function to stem\n",
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "df_stemming = pd.DataFrame(df_reddit['title'].apply(stem_sentences))\n",
    "\n",
    "df_stemming.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_stemming['title']\n",
    "\n",
    "# Instantiate a CountVectorizer with the default hyperparameters.\n",
    "cvec = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fit the vectorizer on our corpus and transform it.\n",
    "X_vec = cvec.fit_transform(X)\n",
    "\n",
    "# Create a dataframe after count vectoriser\n",
    "df_stem_reddit = pd.DataFrame(X_vec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "df_stem_reddit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming reduces the number of columns from 13472 to 10178. A total of 3294 words were reduced to similar forms as the rest. Stemming is the better preprocessing choice as it helps reduce more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the data and implement what we found above.\n",
    "# Stemming\n",
    "df_model = pd.read_csv('./dataset/cleaned.csv') \n",
    "df_model['title'] = pd.DataFrame(df_model['title'].apply(stem_sentences))\n",
    "\n",
    "# Count Vectoriser with the additional stop words\n",
    "X = df_model['title']\n",
    "\n",
    "# Instantiate a CountVectorizer with the new added stop words and min df = 3 and ngram range from 1 to 3.\n",
    "cvec = CountVectorizer(stop_words = stop_words, min_df = 3, ngram_range=(1,3), max_features = 450)\n",
    "\n",
    "# Fit the vectorizer on our corpus and transform it.\n",
    "X_vec = cvec.fit_transform(X)\n",
    "\n",
    "# Create a dataframe after count vectoriser\n",
    "df_vec_model = pd.DataFrame(X_vec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "# Remove the subreddit columns and add back the original columns\n",
    "#df_vec_model.drop(columns =['subreddit'], inplace = True)\n",
    "df_vec_model = pd.concat([df_vec_model, df_model[['subreddit', 'emoji', 'word_count', 'title_length']]], axis=1)\n",
    "\n",
    "df_vec_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable\n",
    "y = df_vec_model.pop('subreddit')\n",
    "X = df_vec_model\n",
    "\n",
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Modelling Mass Fitting\n",
    "We will first attempt to fit a variety of models and pick the top 2 for optimatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "etc = ExtraTreesClassifier(n_jobs=-1)\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "nb = MultinomialNB()\n",
    "ada = AdaBoostClassifier()\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "bag = BaggingClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to find the train and test accuracy scores of the models with 5 fold cross validation\n",
    "def report_error(model, X1, y1, X2, y2):\n",
    "    model.fit(X1, y1)\n",
    "    print('The accuracy train score for', model, 'is', cross_val_score(model, X1, y1, cv=5).mean(),'.')\n",
    "    print('The accuracy test score for',model, 'is', cross_val_score(model, X2, y2, cv=5).mean(),'.')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_error(rfc, X_train, y_train, X_test, y_test)\n",
    "report_error(etc, X_train, y_train, X_test, y_test)\n",
    "report_error(lr, X_train, y_train, X_test, y_test)\n",
    "report_error(nb, X_train, y_train, X_test, y_test)\n",
    "report_error(ada, X_train, y_train, X_test, y_test)\n",
    "report_error(knn, X_train, y_train, X_test, y_test)\n",
    "report_error(bag, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models does not show signs of overfitting. All models except K-Nearest Neigbours have accuracy scores above 85%. The top 2 performing models are LogisticRegression and MultinomialNB with MultinomialNB scoring about 0.01 higher. We will proceed to optimise using grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Model Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the data and implement what we found above.\n",
    "# Stemming\n",
    "df_model = pd.read_csv('./dataset/cleaned.csv') \n",
    "df_model['title'] = pd.DataFrame(df_model['title'].apply(stem_sentences))\n",
    "\n",
    "# Define target variable\n",
    "y = df_model.pop('subreddit')\n",
    "X = df_model['title']\n",
    "\n",
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8967799708339215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.35,\n",
       " 'cvec__max_features': 4000,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up pipeline\n",
    "pipe_nb_c = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words)),\n",
    "    ('nb', MultinomialNB())])\n",
    "\n",
    "# Set up pipeline params\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [3000, 3500, 4000],\n",
    "    'cvec__min_df': [1, 2, 3],\n",
    "    'cvec__max_df': [0.25, 0.35, 0.45],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3)]\n",
    "}\n",
    "\n",
    "# Set up a gridsearch\n",
    "gs_nb_c = GridSearchCV(pipe_nb_c, param_grid = pipe_params, cv=5)\n",
    "\n",
    "# Fit the gridsearch\n",
    "gs_nb_c.fit(X_test, y_test)\n",
    "\n",
    "# Find best score\n",
    "print(gs_nb_c.best_score_)\n",
    "\n",
    "# Find best parameters\n",
    "gs_nb_c.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8962641882220117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.3,\n",
       " 'cvec__max_features': 2000,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up pipeline\n",
    "pipe_lr_c = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words=stop_words)),\n",
    "    ('lr', LogisticRegression())])\n",
    "\n",
    "# Set up pipeline params\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [1000, 1500, 2000],\n",
    "    'cvec__min_df': [1, 2, 3],\n",
    "    'cvec__max_df': [0.25, 0.3, 0.35],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (1,3)]\n",
    "}\n",
    "\n",
    "# Set up a gridsearch\n",
    "gs_lr_c = GridSearchCV(pipe_lr_c, param_grid = pipe_params, cv=5)\n",
    "\n",
    "# Fit the gridsearch\n",
    "gs_lr_c.fit(X_test, y_test)\n",
    "\n",
    "# Find best score\n",
    "print(gs_lr_c.best_score_)\n",
    "\n",
    "# Find best parameters\n",
    "gs_lr_c.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Choice of Model - MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the data and implement what we found above.\n",
    "# Stemming\n",
    "df_model = pd.read_csv('./dataset/cleaned.csv') \n",
    "df_model['title'] = pd.DataFrame(df_model['title'].apply(stem_sentences))\n",
    "\n",
    "# Count Vectoriser with the additional stop words\n",
    "X = df_model['title']\n",
    "\n",
    "# Instantiate a CountVectorizer with the new added stop words and min df = 3 and ngram range from 1 to 3.\n",
    "cvec = CountVectorizer(stop_words = stop_words, min_df = 1, max_df = 0.35,  ngram_range=(1,1), max_features = 3500)\n",
    "\n",
    "# Fit the vectorizer on our corpus and transform it.\n",
    "X_vec = cvec.fit_transform(X)\n",
    "\n",
    "# Create a dataframe after count vectoriser\n",
    "df_vec_model = pd.DataFrame(X_vec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "# Remove the subreddit columns and add back the original columns\n",
    "df_vec_model.drop(columns =['subreddit'], inplace = True)\n",
    "df_vec_model = pd.concat([df_vec_model, df_model[['subreddit', 'emoji', 'word_count', 'title_length']]], axis=1)\n",
    "\n",
    "# Define target variable\n",
    "y = df_vec_model.pop('subreddit')\n",
    "X = df_vec_model\n",
    "\n",
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "report_error(nb, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Choice of Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the data and implement what we found above.\n",
    "# Stemming\n",
    "df_model = pd.read_csv('./dataset/cleaned.csv') \n",
    "df_model['title'] = pd.DataFrame(df_model['title'].apply(stem_sentences))\n",
    "\n",
    "# Count Vectoriser with the additional stop words\n",
    "X = df_model['title']\n",
    "\n",
    "# Instantiate a CountVectorizer with the new added stop words and min df = 3 and ngram range from 1 to 3.\n",
    "cvec = CountVectorizer(stop_words = stop_words, min_df = 2, max_df = 0.3,  ngram_range=(1,1), max_features = 1500)\n",
    "\n",
    "# Fit the vectorizer on our corpus and transform it.\n",
    "X_vec = cvec.fit_transform(X)\n",
    "\n",
    "# Create a dataframe after count vectoriser\n",
    "df_vec_model = pd.DataFrame(X_vec.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "# Remove the subreddit columns and add back the original columns\n",
    "df_vec_model.drop(columns =['subreddit'], inplace = True)\n",
    "df_vec_model = pd.concat([df_vec_model, df_model[['subreddit', 'emoji', 'word_count', 'title_length']]], axis=1)\n",
    "\n",
    "# Define target variable\n",
    "y = df_vec_model.pop('subreddit')\n",
    "X = df_vec_model\n",
    "\n",
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "report_error(lr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salient_words(nb_clf, vect, class_ind):\n",
    "    words = cvec.get_feature_names()\n",
    "    zipped = list(zip(words, nb_clf.feature_log_prob_[class_ind]))\n",
    "    sorted_zip = sorted(zipped, key=lambda t: t[1], reverse=True)\n",
    "    return sorted_zip\n",
    "\n",
    "neg_salient_top_20 = get_salient_words(nb, cvec, 0)[:20]\n",
    "pos_salient_top_20 = get_salient_words(nb, cvec, 1)[:20]\n",
    "\n",
    "pos_salient_top_20\n",
    "neg_salient_top_20"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
