{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Reddit - Webscrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Notebook 2\n",
    "\n",
    "The second notebook will comprise of scrapping the data using API and putting it into 2 dataframes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Data Collection form subreddit Cats\n",
    "Extracting 10000 posts from subreddit Cats and create a dataframe using title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stating the parameters\n",
    "url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "params = {\n",
    "    'subreddit': 'cats',\n",
    "    'size': 100,\n",
    "    'before' : 1641398400 #Local Datetime as 6th Jan 12am\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.get(url, params)\n",
    "res.status_code # check connection status, if 200 means successfully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = res.json()\n",
    "posts = data['data']\n",
    "len(posts) # Check to see if we extracted 100 posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for cats\n",
    "df_cats = pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9995, 83)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate variable for time taken to scrap 9900 posts\n",
    "cats_total_time = 0\n",
    "\n",
    "# Extracting another 9900 posts to get 10000 rows for df_cats\n",
    "for i in range(99):\n",
    "    start_time = time.time()\n",
    "    params = {'subreddit': 'cats', 'size': 100, 'before': posts[-1]['created_utc']}\n",
    "    response = requests.get(url, params)\n",
    "    data = response.json()\n",
    "    posts = data['data']\n",
    "    df_cats = df_cats.append(pd.DataFrame(posts))\n",
    "    end_time = time.time()\n",
    "    exe_time = end_time - start_time\n",
    "    cats_total_time += exe_time\n",
    "    time.sleep(2)\n",
    "\n",
    "# Check if we have 10000 posts\n",
    "df_cats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not manage to scrap 10,000 posts. This might be some posts been deleted and the gap is not been removed. However we did obtain close to 10,000 post and is sufficient for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Data Collection form subreddit Dogs\n",
    "Extracting 1000 posts from subreddit Dogs and create a dataframe using title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the parameters for dogs\n",
    "params = {\n",
    "    'subreddit': 'dogs',\n",
    "    'size': 100,\n",
    "    'before' : 1641398400 #Local Datetime as 6th Jan 12am\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for dogs\n",
    "res = requests.get(url, params)\n",
    "data = res.json()\n",
    "posts = data['data']\n",
    "df_dogs = pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9996, 78)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate variable for time taken to scrap 9900 posts\n",
    "dogs_total_time = 0\n",
    "\n",
    "# Extracting another 9900 posts to get 10000 rows for df_dogs\n",
    "for i in range(99):\n",
    "    start_time = time.time()\n",
    "    params = {'subreddit': 'dogs', 'size': 100, 'before': posts[-1]['created_utc']}\n",
    "    response = requests.get(url, params)\n",
    "    data = response.json()\n",
    "    posts = data['data']\n",
    "    df_dogs = df_dogs.append(pd.DataFrame(posts))\n",
    "    end_time = time.time()\n",
    "    exe_time = end_time - start_time\n",
    "    dogs_total_time += exe_time\n",
    "    time.sleep(2)\n",
    "    \n",
    "# Check if we have 10000 posts\n",
    "df_dogs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only manage to scrap 9996 posts. This is only 1 more than the scraps from the cat subreddit. The 2 scraps should be similar in size and not cause one class to overwhelm another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Usefulness of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_is_blocked</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>...</th>\n",
       "      <th>media</th>\n",
       "      <th>media_embed</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>secure_media_embed</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>category</th>\n",
       "      <th>banned_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>BeepBeep-Richie</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_33zkxlgz</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>__katsby</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_40jfdgd6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>TinyTotoro3</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_676c778q</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>clickbaitbabe</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_gvp1d1m</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>TinyTotoro3</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_676c778q</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments           author author_flair_css_class  \\\n",
       "0            []                False  BeepBeep-Richie                   None   \n",
       "1            []                False         __katsby                   None   \n",
       "2            []                False      TinyTotoro3                   None   \n",
       "3            []                False    clickbaitbabe                   None   \n",
       "4            []                False      TinyTotoro3                   None   \n",
       "\n",
       "  author_flair_richtext author_flair_text author_flair_type author_fullname  \\\n",
       "0                    []              None              text     t2_33zkxlgz   \n",
       "1                    []              None              text     t2_40jfdgd6   \n",
       "2                    []              None              text     t2_676c778q   \n",
       "3                    []              None              text      t2_gvp1d1m   \n",
       "4                    []              None              text     t2_676c778q   \n",
       "\n",
       "   author_is_blocked author_patreon_flair  ... media media_embed  \\\n",
       "0              False                False  ...   NaN         NaN   \n",
       "1              False                False  ...   NaN         NaN   \n",
       "2              False                False  ...   NaN         NaN   \n",
       "3              False                False  ...   NaN         NaN   \n",
       "4              False                False  ...   NaN         NaN   \n",
       "\n",
       "   secure_media  secure_media_embed  author_flair_background_color  \\\n",
       "0           NaN                 NaN                            NaN   \n",
       "1           NaN                 NaN                            NaN   \n",
       "2           NaN                 NaN                            NaN   \n",
       "3           NaN                 NaN                            NaN   \n",
       "4           NaN                 NaN                            NaN   \n",
       "\n",
       "  author_flair_text_color author_flair_template_id author_cakeday category  \\\n",
       "0                     NaN                      NaN            NaN      NaN   \n",
       "1                     NaN                      NaN            NaN      NaN   \n",
       "2                     NaN                      NaN            NaN      NaN   \n",
       "3                     NaN                      NaN            NaN      NaN   \n",
       "4                     NaN                      NaN            NaN      NaN   \n",
       "\n",
       "  banned_by  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 5 rows shows empty posts for selftext. Lets examine the number of empty posts in df_cats using value_counts below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             8593\n",
       "[removed]     180\n",
       "[deleted]      14\n",
       "Name: selftext, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cats['selftext'].value_counts().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_is_blocked</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>...</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>preview</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_list</th>\n",
       "      <th>url_overridden_by_dest</th>\n",
       "      <th>thumbnail_height</th>\n",
       "      <th>thumbnail_width</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>banned_by</th>\n",
       "      <th>edited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Substantial-Koala-32</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_gp3egwzs</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Unsd</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_4d18z6g4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Cubs017</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_rt2lm</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>stuffiwannaknow</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_cbmsbxv</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>seeyouinthesun</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_ho8lowto</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments                author  \\\n",
       "0            []                False  Substantial-Koala-32   \n",
       "1            []                False                  Unsd   \n",
       "2            []                False               Cubs017   \n",
       "3            []                False       stuffiwannaknow   \n",
       "4            []                False        seeyouinthesun   \n",
       "\n",
       "  author_flair_css_class author_flair_richtext author_flair_text  \\\n",
       "0                   None                    []              None   \n",
       "1                   None                    []              None   \n",
       "2                   None                    []              None   \n",
       "3                   None                    []              None   \n",
       "4                   None                    []              None   \n",
       "\n",
       "  author_flair_type author_fullname  author_is_blocked author_patreon_flair  \\\n",
       "0              text     t2_gp3egwzs              False                False   \n",
       "1              text     t2_4d18z6g4              False                False   \n",
       "2              text        t2_rt2lm              False                False   \n",
       "3              text      t2_cbmsbxv              False                False   \n",
       "4              text     t2_ho8lowto              False                False   \n",
       "\n",
       "   ... post_hint preview  crosspost_parent  crosspost_parent_list  \\\n",
       "0  ...       NaN     NaN               NaN                    NaN   \n",
       "1  ...       NaN     NaN               NaN                    NaN   \n",
       "2  ...       NaN     NaN               NaN                    NaN   \n",
       "3  ...       NaN     NaN               NaN                    NaN   \n",
       "4  ...       NaN     NaN               NaN                    NaN   \n",
       "\n",
       "   url_overridden_by_dest thumbnail_height thumbnail_width distinguished  \\\n",
       "0                     NaN              NaN             NaN           NaN   \n",
       "1                     NaN              NaN             NaN           NaN   \n",
       "2                     NaN              NaN             NaN           NaN   \n",
       "3                     NaN              NaN             NaN           NaN   \n",
       "4                     NaN              NaN             NaN           NaN   \n",
       "\n",
       "  banned_by  edited  \n",
       "0       NaN     NaN  \n",
       "1       NaN     NaN  \n",
       "2       NaN     NaN  \n",
       "3       NaN     NaN  \n",
       "4       NaN     NaN  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dogs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[removed]    792\n",
       "              68\n",
       "[deleted]     43\n",
       "Name: selftext, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dogs['selftext'].value_counts().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Usefulness of Title and Selftext column</u>\n",
    "* All the titles from the cats reddit and the dogs reddit are not empty.\n",
    "* The selftext from cats has 8593 blanks, 180 removed and 14 deleted which totals to 8787 missing data.\n",
    "* The blanks are likely due to posting of pictures which is not helpful to us.\n",
    "* The selftext from dogs has 68 blanks and 792 removed and 43 deleted which totals to 862 missing data.\n",
    "\n",
    "This suggests that using selftext from the cat reddit will not generate sufficient data with 1000 scraps. For this project, we will only use body and not selftext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Relevance of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cats['title']\n",
    "y = df_cats['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(ngram_range=(1, 1), stop_words='english')\n",
    "X = cvec.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cats_title = pd.DataFrame(X.todense(), columns = cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat       2899\n",
       "cats       690\n",
       "new        614\n",
       "just       500\n",
       "like       429\n",
       "year       428\n",
       "little     402\n",
       "kitten     345\n",
       "old        305\n",
       "kitty      290\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cats_title.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dogs['title']\n",
    "y = df_dogs['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(ngram_range=(1, 1), stop_words='english')\n",
    "X = cvec.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dogs_title = pd.DataFrame(X.todense(), columns = cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dog       5264\n",
       "dogs      1111\n",
       "help       878\n",
       "puppy      618\n",
       "old        425\n",
       "advice     397\n",
       "need       337\n",
       "breed      314\n",
       "does       304\n",
       "new        299\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dogs_title.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Relevance of the top 10 Highest Occurence Words</u>\n",
    "* The top words for cats reddit that are relevant are cat, cats, kitty, kitten\n",
    "* The top words for dogs reddit that are relevant are dog, dogs, puppy, breed\n",
    "\n",
    "The rest of the top words for cats and dogs are too general and can appear in both reddits. After lementization, we probably will be left with 2 relevnat words for cats and 3 for dogs. Although this is suggestive that it will be harder to identify which reddit a post is from, however general words can also be learned by the machine. For example, the word love might mean cat owners post about how they love their cats while the word food might mean dog owners are more concern with what they feed their dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Storage Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial scrap of 100 posts are done and a dataframe is created for each subreddit and a for loop is utilised to scrap the remaining 9900 posts and append to the dataframe. A function is not required as we are only repeating the process twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Server Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total execution time to scrap 9900 posts from cats subreddit is 10.65 minutes\n",
      " and the mean time for each scrap is 6.45 seconds\n",
      "The total execution time to scrap 9900 posts from dogs subreddit is 9.03 minutes\n",
      " and the mean time for each scrap is 5.47 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f'The total execution time to scrap 9900 posts from cats subreddit is {cats_total_time/60:0.2f} minutes\\n and the mean time for each scrap is {cats_total_time/99:0.2f} seconds')\n",
    "print(f'The total execution time to scrap 9900 posts from dogs subreddit is {dogs_total_time/60:0.2f} minutes\\n and the mean time for each scrap is {dogs_total_time/99:0.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average time to scrap 100 posts from both reddits is under 7 seconds and the total time taken to scrap 9900 posts from both subreddit is less than 11 minutes each. As the mean time is reasonable small, it shows that the server is not overloaded. \n",
    "\n",
    "A 2 seconds lag is also implemented in each loop to delay the execution of the next to not tax on the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the datasets to csv so as not to scrap again.\n",
    "df_cats.to_csv('./dataset/cats.csv', index = False)\n",
    "df_dogs.to_csv('./dataset/dogs.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
