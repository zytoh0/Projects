{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a soup object from the home page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://pages.git.generalassemb.ly/rldaggie/for-scraping/'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrape the home page soup for every restaurant\n",
    "\n",
    "Note: Your best bet is to create a list of dictionaries, one for each restaurant. Each dictionary contains the restaurant's name and path from the `href`. The result of your scrape should look something like this:\n",
    "\n",
    "```python\n",
    "restaurants = [\n",
    "    {'name': 'A&W Restaurants', 'href': 'restaurants/1.html'}, \n",
    "    {'name': \"Applebee's\", 'href': 'restaurants/2.html'},\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'A&W Restaurants', 'href': 'restaurants/1.html'},\n",
       " {'name': \"Applebee's\", 'href': 'restaurants/2.html'},\n",
       " {'name': \"Arby's\", 'href': 'restaurants/3.html'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolate the table that has all the restaurant links\n",
    "restaurants_table = soup.find('table', {'id': 'restaurants'})\n",
    "\n",
    "# Create an empty list\n",
    "restaurants = []\n",
    "\n",
    "# Loop through each link in the restaurants table\n",
    "for restaurant_link in restaurants_table.find_all('a'):\n",
    "    # Start with an empty dictionary\n",
    "    restaurant = {}\n",
    "    \n",
    "    # Add name\n",
    "    restaurant['name'] = restaurant_link.text\n",
    "    \n",
    "    # Add href\n",
    "    restaurant['href'] = restaurant_link['href']\n",
    "    \n",
    "    # Add restaurant to our list of restaurants\n",
    "    restaurants.append(restaurant)\n",
    "restaurants[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Using the `href`, scrape each restaurant's page and create a single list of food dictionaries.\n",
    "\n",
    "Your list of foods should look something like this:\n",
    "```python\n",
    "foods = [\n",
    "    {\n",
    "        'calories': '0',\n",
    "        'carbs': '0',\n",
    "        'category': 'Drinks',\n",
    "        'fat': '0',\n",
    "        'name': 'A&W® Diet Root Beer',\n",
    "        'restaurant': 'A&W Restaurants'\n",
    "    },\n",
    "    {\n",
    "        'calories': '0',\n",
    "        'carbs': '0',\n",
    "        'category': 'Drinks',\n",
    "        'fat': '0',\n",
    "        'name': 'A&W® Diet Root Beer',\n",
    "        'restaurant': 'A&W Restaurants'\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "**Note**: Remove extra white space from each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping A&W Restaurants\n",
      "Scraping Applebee's\n",
      "Scraping Arby's\n",
      "Scraping Atlanta Bread Company\n",
      "Scraping Bojangle's Famous Chicken 'n Biscuits\n",
      "Scraping Buffalo Wild Wings\n",
      "Scraping Burger King\n",
      "Scraping Captain D's\n",
      "Scraping Carl's Jr.\n",
      "Scraping Charley's Grilled Subs\n",
      "Scraping Chick-fil-A\n",
      "Scraping Chili's\n",
      "Scraping Chipotle Mexican Grill\n",
      "Scraping Church's\n",
      "Scraping Corner Bakery Cafe\n",
      "Scraping Dairy Queen\n",
      "Scraping Denny's\n",
      "Scraping El Pollo Loco\n",
      "Scraping FATZ\n",
      "Scraping Fazoli's\n",
      "Scraping Five Guys Burgers and Fries\n",
      "Scraping Golden Chick\n",
      "Scraping Hardee's\n",
      "Scraping IHOP\n",
      "Scraping In-N-Out Burger\n",
      "Scraping Jack in the Box\n",
      "Scraping Jimmy Johns\n",
      "Scraping Joe's Crab Shack\n",
      "Scraping KFC\n",
      "Scraping McDonald's\n",
      "Scraping O'Charley's\n",
      "Scraping Olive Garden\n",
      "Scraping Outback Steakhouse\n",
      "Scraping Panda Express\n",
      "Scraping Panera Bread\n",
      "Scraping Popeye's\n",
      "Scraping Quiznos\n",
      "Scraping Red Robin Gourmet Burgers\n",
      "Scraping Romano's Macaroni Grill\n",
      "Scraping Ruby Tuesday\n",
      "Scraping Subway\n",
      "Scraping Taco Bell\n"
     ]
    }
   ],
   "source": [
    "# Start with an empty list\n",
    "foods = []\n",
    "\n",
    "# Loop through each restaurant in the previous step\n",
    "for restaurant in restaurants:\n",
    "    print('Scraping {}'.format(restaurant['name']))\n",
    "    \n",
    "    href = restaurant['href']\n",
    "    restaurant_url = f'https://pages.git.generalassemb.ly/rldaggie/for-scraping/{href}'\n",
    "    \n",
    "    # Use requests library to get the content from each restaurant page\n",
    "    restaurant_res = requests.get(restaurant_url)\n",
    "    \n",
    "    # Create soup object from restauarant html\n",
    "    restaurant_soup = BeautifulSoup(restaurant_res.content, 'lxml')\n",
    "    \n",
    "    # Isolate the foods table from restaurant page\n",
    "    table = restaurant_soup.find('table')\n",
    "    \n",
    "    # Loop through each row in the tbody of the restaurants table\n",
    "    for row in table.find('tbody').find_all('tr'):\n",
    "        # We'll use almost all the <td /> tags for each row, might as well create a variable\n",
    "        cells = row.find_all('td')\n",
    "        \n",
    "        # Start with an empty food dictionary\n",
    "        food = {}\n",
    "        \n",
    "        # Add the restaurant's name (No need for the slug, that was just for scraping purposes)\n",
    "        food['restaurant'] = restaurant['name']\n",
    "        \n",
    "        # Add food name from firs cell\n",
    "        food['name'] = cells[0].text\n",
    "        \n",
    "        # Add category, note the .strip() for removing white space\n",
    "        food['category'] = cells[1].text.strip()\n",
    "        \n",
    "        # Add calories \n",
    "        food['calories'] = cells[2].text\n",
    "        \n",
    "        # Add fat\n",
    "        food['fat'] = cells[3].text\n",
    "        \n",
    "        # Add carbs\n",
    "        food['carbs'] = cells[4].text\n",
    "        \n",
    "        # Add the food dictionary to our list of foods\n",
    "        foods.append(food)\n",
    "        \n",
    "    # Be courteous and throttle your scrapes!\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a pandas DataFrame from your list of foods\n",
    "\n",
    "**Note**: Your DataFrame should have 5,131 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = pd.DataFrame(foods)\n",
    "foods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Export to csv\n",
    "\n",
    "**Note:** Don't export the index column from your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods.to_csv('foods.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
